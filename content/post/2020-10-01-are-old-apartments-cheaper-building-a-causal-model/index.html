---
title: Are old apartments cheaper?...Building a statistical model!
author: ''
date: '2020-10-01'
slug: []
categories: []
tags: [housing, regression]
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p></br></p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In <a href="https://tfarley10.github.io/2020/09/26/are-old-apartments-cheaper-part-1/">part 1</a> while exploring the dataset I stated two a priori beliefs about the relationship between building age and rental price. First, especially when taking selection bias, this relationship is unlikely to be linear (or log-linear in this case). Second, I suggested that the relationship was likely to be different in different types of neighborhoods. In this post, I’ll start by specifying a statistical model that addresses the mediating effects of ‘Neighborhood Income level’, non-linearities, and confounding variable bias. I’ll fit this model and then interpret it using conditional effects plots derived from the posterior distribution.</p>
<div id="specifying-my-statistical-model" class="section level3">
<h3>Specifying my statistical model</h3>
<p>The first challenge in building this model is to address non-linearities in the relationship between age and rental price. Splines offer a good solution for modeling non-linear effects. One common approach to modeling non-linearities like this is to use polynomial regression. However, polynomial regression can only model non-monotonic relationships– relationships in which the slope is always changing. This constraint of polynomial models can yield <a href="https://twitter.com/WhiteHouseCEA/status/1257680258364555264">outrageous predictions</a> and <a href="https://statmodeling.stat.columbia.edu/2019/06/30/what-if-the-authors-of-that-regression-discontinuity-paper-had-only-reported-their-local-linear-model-results-with-no-graph/">invalid hypothesis tests</a><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Splines are a great tool and there are a ton of great resources out there for applying them in R!</p>
</div>
<div id="more-formally-my-statistical-model-looks-like-this" class="section level3">
<h3>More formally my statistical model looks like this:</h3>
<p></br></p>
<p><span class="math inline">\(R_i\)</span>, the monthly rental price for unit <span class="math inline">\(i\)</span>, is log-normally distributed conditional on <span class="math inline">\(\mu_i\)</span>, the mean for unit <span class="math inline">\(i\)</span> and the variance <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display">\[\begin{align*}
R_i   \sim &amp; LogNormal(\mu_i, \sigma^2) \\
\mu_i    = &amp; \alpha_{BID[i]} + \sum^{J}_{j=1}w_jF_{j,BID[i]} + \beta_{size}Size_i + \beta_{bedrooms}Bedrooms_i
\end{align*}\]</span></p>
<p><span class="math inline">\(\alpha_{BID[i]}\)</span> is the partially pooled intercept for the 3 income buckets.</p>
<div id="the-splines-part-of-the-model" class="section level4">
<h4>The splines part of the model</h4>
<p>Splines fit non-linear relationships by using sets of synthetic functions, basis functions, assigning weights to each function and then adding them all up. These additive combinations forms forms wiggly relationships from linear functions. The <span class="math inline">\(\sum^{J}_{j=1}w_jF_{j,BID[i]}\)</span> term in my linear model illustrates this process. Where the <span class="math inline">\(jth\)</span> basis function <span class="math inline">\(F_j\)</span> is multiplied by the associated weight <span class="math inline">\(w_j\)</span>, and then all <span class="math inline">\(j\)</span> functions are added up for every observation <span class="math inline">\(i\)</span>. The key to splines is that all of the basis functions are linear, but, when you add them together, they don’t look linear.</p>
</div>
<div id="weakly-informative-priors" class="section level4">
<h4>Weakly informative priors</h4>
<p><span class="math display">\[\begin{align*}
\beta_{size}, \beta_{bedrooms}  \sim &amp; Normal(0,1) \\
\alpha_{BID[i]} \sim &amp; Normal(\gamma, \tau) \\
\gamma \sim &amp; Normal(0,1.5) \\
\tau  \sim &amp; Exponential(1) \\
\lambda   \sim &amp; StudentT(3,0,10) \\
\end{align*}\]</span></p>
<p><span class="math inline">\(\beta_{size}\)</span> and <span class="math inline">\(\beta_{bedrooms}\)</span> are the parameters for the slope coefficients that have been centered at zero and scaled to have standard deviation = 1. <span class="math inline">\(\alpha_{BID[i]}\)</span>, the partially-pooled parameter for the income bucket intercepts and has two hyper-priors: <span class="math inline">\(\gamma\)</span> the parameter for the population intercept and <span class="math inline">\(\tau\)</span> the scale parameter for varying intercepts. Finally, <span class="math inline">\(\lambda\)</span> is the scale parameter for the spline term which essentially learn’s how ‘wiggly’ the relationship is (I cant really figure out how to incorporate this into my statistical model).</p>
<ul>
<li>R code for model specification</li>
</ul>
<pre class="r"><code># create brms formula
smooth_form &lt;- 
  brms::bf( 
           rent_lc ~ 1 + 
             s(age, by = income_bucket, bs = &quot;fs&quot;) + 
                 bedrooms + size_lc + (1|income_bucket) 
    )</code></pre>
<ul>
<li>R code {brms} for my priors</li>
</ul>
<pre class="r"><code># priors for smooth model
priors_smooth &lt;- c(  # ß_bedroom, ß_size
                     prior(normal(0, 1.5),       class = b),
                     # population intercept
                     prior(normal(0,1.5),        class = Intercept),
                     # population variance, gamma
                     prior(exponential(1),       class = sigma),
                     # fixed effects variance, tau
                     prior(exponential(1),       class = sd),
                     # wiggliness of the smooths
                     prior(student_t(3, 0, 2.5), class = sds))</code></pre>
</div>
</div>
<div id="actually-fitting-the-model" class="section level3">
<h3>Actually fitting the model</h3>
<p>At this point I have read in the Streeteasy dataset (<code>ste</code>) from <a href="https://raw.githubusercontent.com/Codecademy/datasets/master/streeteasy/streeteasy.csv">the Codeacademy github repo</a>. I’ll log-transform the <code>rent</code> to meet the additivity assumption for linear regression models. I also log-transform <code>size</code> independent variable for the square footage of an apartment.</p>
<p>To fit the model, I use the {brms<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>} package in R. {brms} is essentially a warpper function for the probabilistic-programming language Stan<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. Stan is (I’m told) the latest-and-greatest in Markov chain Monte Carlo (MCMC) methods for sampling from a posterior distribution. Stan is well known for it’s implementation of a particularly efficient algorithm called the No U-turn sampler (NUTS). While getting into the weeds of probabilistic programming is outside the scope of this project (and above my pay grade), essentially NUTS uses clever tricks to avoid ‘getting stuck’ in certain areas of the posterior distribution.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> I show the code for specifying my model in {brms}, combined with the formula and priors set above, the model is ready to be fit.</p>
<div id="brms-code-for-fitting-my-model" class="section level4">
<h4>{brms} code for fitting my model</h4>
<pre class="r"><code>set.seed(1020)
# fit the smooth model
smooth_mod &lt;- 
  brms::brm(
             formula = smooth_form,
             data    = ste,
             warmup  = 1000,
             iter    = 3000,
             prior   = priors_smooth,
             # this saves .rds object
             file    = &quot;smooth_model&quot;,
             chains  = 4,
             cores   = 4,
             # NUTS specific parameter for avoiding divergent transitions
             control = list(adapt_delta = 0.90)
  )</code></pre>
</div>
</div>
</div>
<div id="conditional-effects" class="section level2">
<h2>Conditional effects</h2>
<div id="constructing-the-conditional-effects-plot" class="section level4">
<h4>Constructing the conditional effects plot</h4>
<p>My goal is to use this model to interpret the conditional effects of age on the price of rent. To do this I construct a fake dataset of 3,000 units, 1,000 for each income level, of 1,050 sq. ft, 2 bedroom apartment units and 50 evenly spaced age intervals. Each line-segment is a draw from the posterior. I also fit a linear-model with essentially equivalent specifications in order to have a benchmark for my splines-model.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/plot_draws-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="interpretation" class="section level3">
<h3>Interpretation</h3>
<p>The two models are clearly very different. While my splines model shows that the relationship is clearly not linear, I’m not particularly satisfied with either model.</p>
<div id="splines-reveal-cohort-effects" class="section level4">
<h4>Splines reveal cohort effects</h4>
<p>It’s really hard to make sense of the splines model here, the shape that it takes in each income-bucket doesn’t really make any sense right off the bat. Consider the conditional means in the <em>low-income</em> bucket. Strangely, the relationship between age and rent seems to increase before a sharp decrease. This doesn’t make sense from a selection-bias point of view. With the selection bias story, we would expect a ‘V’ or a ‘U’ shape not a ‘W’. Furthermore, the model looks <em>uncertain</em> about the relationship in this age-range. Let’s look at the density of low-income apartments in this age range:</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>The picture becomes a little bit more clear whats happening here, at least why the model is so uncertain for unit between 25 and 50 years old in low income neighborhoods– the data is very sparse in the age range. Now as I mentioned in my post building <a href="https://tfarley10.github.io/2020/09/27/are-old-apartments-cheaper-building-a-causal-model/">causal models</a> since the dataset is only from 2014, age is perfectly correlated with cohort, therefore we are subject to omitted variable bias. In this case it’s very useful to think about what was <em>different</em> about low-income neighborhoods between 1964 and 1989, doing so will shed some light on the sparsity of the data in this time period. The ’70s were a terrible time for New York, the city was facing a <a href="https://indypendent.org/2020/07/kim-phillips-fein-what-nycs-fiscal-crisis-of-the-1970s-can-teach-us-today/">fiscal crisis</a>, people were leaving the city in droves and, in the poorest of neighborhoods abandonment was so bad that it was not uncommon for landowners to <a href="https://nypost.com/2010/05/16/why-the-bronx-burned/">just burn their builings down</a>.</p>
</div>
<div id="how-are-these-cohort-effects-bias-my-models" class="section level4">
<h4>How are these cohort effects bias my models</h4>
<p>I have shown how cohort effects effect my model by making the sample sparse between c. 1965– c. 1990. However the sparsity only suggests that my model will be more uncertain in this age range in low income neighborhoods a complete analysis of these cohort effects will take a lot more work and probably more data. A straightforward way out of this problem is to attain data for consecutive years. The more years of data used, the more these 1970’s cohort effects could be spread out across age ranges.</p>
</div>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>In this post, using structure and insights gathered from builing my causal models, I specify my statistical model with both mathematical notation and code. I then fit the model and construct a conditional means plot by sampling from the posterior distribution of this model and making predictions with this fake data.</p>
<p>I think the most important take-away from this is that the splines model was flexible enough to reveal the cohort bias I talk about in the final section. In contrast, the linear model hides these issues and severely biases the interpretation of the model. My next post is on model performance and using more formal metrics to evaluate both in-sample and out-of-sample performance of these models. In this post, my identification issues have become even more clear so it will be necessary to incorporate these issues into my discussion on model performance.</p>
<p><a href="https://tfarley10.github.io/2020/10/04/are-old-apartments-cheaper-model-comparison/">Next Post: Model Comparisons</a></p>
<p><a href="https://github.com/tfarley10/blog_notebooks/blob/master/2020-10-03-are-old-apartments-cheaper-part-3.Rmd">Notebook for this post</a></p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>McElreath, Richard. Statistical Rethinking a Bayesian Course with Examples in R and Stan. CRC Press, 2020.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Bürkner P (2017). “brms: An R Package for Bayesian Multilevel Models Using Stan.” Journal of Statistical Software, 80(1), 1–28. doi: 10.18637/jss.v080.i01.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Stan Development Team. 2018. The Stan Core Library, Version 2.18.0. <a href="http://mc-stan.org" class="uri">http://mc-stan.org</a><a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/" class="uri">https://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/</a><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
